{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS3: Deep learning\n",
    "\n",
    "In this problem set, you will experiment with fully-connected neural networks.\n",
    "\n",
    "To start with, let's load the \"breast cancer\" data set from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "y = np.matrix(data.target).T\n",
    "X = np.matrix(data.data)\n",
    "M = X.shape[0]\n",
    "N = X.shape[1]\n",
    "\n",
    "# Normalize each input feature\n",
    "def normalize(X):\n",
    "    M = X.shape[0]\n",
    "    XX = X - np.tile(np.mean(X,0),[M,1])\n",
    "    XX = np.divide(XX, np.tile(np.std(XX,0),[M,1]))\n",
    "    return XX\n",
    "\n",
    "XX = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's represent a fully-connected neural network by two arrays W and b containing the weights and biases for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = 10\n",
    "h2 = 5\n",
    "h3 = 3\n",
    "W = [[]]\n",
    "b = [[]]\n",
    "\n",
    "# units is the number of units each layer, e.g. [N,h,1] is input layer N unit, second layer h ...\n",
    "def initializer(no_of_layers, units):\n",
    "    for i in range(0,no_of_layers-1):\n",
    "        W.append(np.random.normal(0,0.1,[units[i],units[i+1]]))\n",
    "        b.append(np.random.normal(0,0.1,[units[i+1],1]))\n",
    "        \n",
    "        \n",
    "initializer(5,[N,h1,h2,h3,1])\n",
    "\n",
    "L = len(W)-1\n",
    "\n",
    "def act(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "#activation derivative\n",
    "def actder(z):\n",
    "    az = act(z)\n",
    "    prod = np.multiply(az,1-az)\n",
    "    return prod\n",
    "\n",
    "def ff(x,W,b):\n",
    "    L = len(W)-1\n",
    "    a = x\n",
    "    for l in range(1,L+1):\n",
    "        z = W[l].T*a+b[l]\n",
    "        a = act(z)\n",
    "    return a\n",
    "\n",
    "def loss(y,yhat):\n",
    "    return -((1-y) * np.log(1-yhat) + y * np.log(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Write Python code to separate $\\texttt{X},\\texttt{y}$ randomly into a training set containing 80% of the data and a validation set consisting of the remaining 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.random.permutation(range(0,len(X)))\n",
    "split_index = int(len(X)*0.8)\n",
    "X_train = XX[order[0:split_index]]\n",
    "y_train = y[order[0:split_index]]\n",
    "X_validate = XX[order[split_index:]]\n",
    "y_validate = y[order[split_index:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Beginning with the training code we wrote together in class, write Python code to execute backpropagation with mini-batch size 1 on the training set, and plot the training loss and validation loss as a function of training iteration. Show the plot in this sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Wait...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-089775d4b90d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please Wait...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "def ff(L,X_t,split_index,y_t=None,type=\"trainer\"):\n",
    "    loss_this_iter = 0\n",
    "    order = np.random.permutation(split_index)\n",
    "\n",
    "    for i in range(0,split_index):\n",
    "        # Grab the pattern order[i]\n",
    "\n",
    "        x_this = X_t[order[i],:].T\n",
    "        if y_t.all:\n",
    "            y_this = y_t[order[i],0]\n",
    "\n",
    "        # Feed forward step\n",
    "\n",
    "        a = [x_this]\n",
    "        z = [[]]\n",
    "        delta = [[]]\n",
    "        dW = [[]]\n",
    "        db = [[]]\n",
    "        \n",
    "        for l in range(1,L+1):\n",
    "            z.append(W[l].T*a[l-1]+b[l])\n",
    "            a.append(act(z[l]))\n",
    "            # Just to give arrays the right shape for the backprop step\n",
    "            delta.append([]); dW.append([]); db.append([])\n",
    "            \n",
    "        loss_this_pattern = loss(y_this, a[L][0,0])\n",
    "        loss_this_iter = loss_this_iter + loss_this_pattern\n",
    "        \n",
    "        if type == \"trainer\":\n",
    "            backprop(L,a,z,delta,dW,db,y_this)\n",
    "\n",
    "    return loss_this_iter\n",
    "        \n",
    "        \n",
    "# Backprop step\n",
    "def backprop(L,a,z,delta,dW,db,y_this):        \n",
    "    delta[L] = a[L] - y_this\n",
    "    for l in range(L,0,-1):\n",
    "        db[l] = delta[l].copy()\n",
    "        dW[l] = a[l-1] * delta[l].T\n",
    "        if l > 1:\n",
    "            delta[l-1] = np.multiply(actder(z[l-1]), W[l] * delta[l])\n",
    "\n",
    "    for l in range(1,L+1):            \n",
    "        W[l] = W[l] - alpha * dW[l]\n",
    "        b[l] = b[l] - alpha * db[l]\n",
    "\n",
    "#this function is for testing and validation     \n",
    "def trainer(W,b,L,X_t,y_t,split_index,X_v,y_v,max_iter=1000):\n",
    "    #loss in training set every iteration\n",
    "    loss_every_iter_train = []\n",
    "    \n",
    "    #loss in validation set every iteration\n",
    "    loss_every_iter_val = []\n",
    "    \n",
    "    for iter in range(0, max_iter):\n",
    "        \n",
    "        #for training set\n",
    "        loss_this_iter = ff(L,X_t,split_index,y_t,\"trainer\")\n",
    "        loss_every_iter_train.append(loss_this_iter)\n",
    "        \n",
    "        #for validation set\n",
    "        loss_this_iter = ff(L,X_v,len(X)-split_index,y_v)\n",
    "        loss_every_iter_val.append(loss_this_iter)\n",
    "    \n",
    "    #for training set\n",
    "    plt.plot(range(0,len(loss_every_iter_train)),loss_every_iter_train)\n",
    "    plt.show()\n",
    "    \n",
    "    #for validation set\n",
    "    plt.plot(range(0,len(loss_every_iter_val)),loss_every_iter_val)\n",
    "    plt.show()\n",
    "    print(loss_every_iter_val)\n",
    "\n",
    "print(\"Please Wait...\")\n",
    "trainer(W,b,L,X_train,y_train,split_index,X_validate,y_validate,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Perform several experiments with different numbers of layers and different numbers of hidden units. Demonstrate the phenomenon of overtraining, make a table showing the training and validation set performance of each of your models, and make a recommendation about which model is best based on validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h1 = 10\n",
    "# h2 = 5\n",
    "\n",
    "# W = [[]]\n",
    "# b = [[]]\n",
    "\n",
    "# initializer(4,[N,h1,h2,1])\n",
    "\n",
    "# L = len(W)-1\n",
    "\n",
    "# trainer(W,b,L,X_train,y_train,split_index,X_validate,y_validate,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results table and discussion goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Modify the backpropagation procedure to use mini-batches of a few different sizes such as 10, 20, and 40. Take care that each mathematical operation is efficient (avoid any for loops over the examples in a mini-batch). Repeat your experiments and report the results. Do you observe any differences in terms of accuracy and number of iterations to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results table and discussion goes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Modify the model to use the ReLU activation function in the hidden layers rather than logistic sigmoid. Repeat your experiments and report the results. Do you observe any differences in terms of accuracy and number of iterations to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results table and discussion goes here.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
